---
title: Behind the scenes of a RAG based LLM powered QAChatBot - LangChain - GPT4
date: '02-21-2024'
tags: [Tech, Data, ML]
draft: false
summary: Introduction to RAGS - Elevating QA Chatbots with Advanced Language Models
---

In this article we're gonna look at how RAG based LLM powered QA Chatbots work,
Thanks to Alex Litvinov for such informative article on [Building ZoomcampQABot](https://www.youtube.com/watch?v=QhFLeZV-PVk)

Let's start with getting a better understanding of what a `RAG` is.

We know in the recent years how Large language models have undergone a remarkable boom, fundamentally reshaping the landscape
of natural language processing (NLP) and artificial intelligence. These models, such as OpenAI's GPT series and Google's BERT, are
characterized by their vast size, often comprising billions or even trillions of parameters. What sets LLMs apart
is their ability to understand and generate human-like text at an unprecedented scale and complexity.

So, what if we have custom data that we want our language model to be trained on. This can be accomplished by using
a `RAG` model.

## Retrieval augmented generation (RAG)

or RAG, is an architectural approach that can improve the efficacy of
large language model (LLM) applications by leveraging custom data.
This is done by retrieving data/documents relevant to a question or task and providing them
as context for the LLM. RAG has shown success in support chatbots and Q&A systems
that need to maintain up-to-date information or access domain-specific knowledge.

<img className="inline" src="/static/images/Blog/langchain-pipeline.png" alt="Langchain-Pipeline" />

It basically consists of three main components:

- Ingestion \
  Done offline, where the data is ingested and indexed, Before we get the question from user. We collect the data from
  the data source, break it to smaller chunks and then embbbed it to a vector database. This allows us to run `semantic search` (takes meaning into account).
- Retrieval \
  Get the query from the user, and then run a semantic search on the `vector database` to get the most relevant documents.
- Generation \
  Consturct the final prompt by combining the query and the retrieved documents, and then pass it to the `LLM` to get the answer.

RAG can be seen as prompt engineering.

<img className="inline" src="/static/images/Blog/rag-promt.png" alt="rag-prompt" />

Now that we know how RAG works, let's look into what `LangChain` is and how it uses RAG to build a Q&A Chatbot.

## What is LangChain?

Langchain allows you to connect a `LLM` like `GPT4` to your own source of data. Like referencing an entire database of
your data. LangChain helps us build a pipeline shown in the image above, that helps us to provide answers to our questions,
as well as take actions based on the answers.
